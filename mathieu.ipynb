{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ec6448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "base_path = os.path.expanduser(\"/Users/mariannelado-roy/data_grid2op\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ed1795",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6675b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grid2op.Reward import L2RPNReward\n",
    "from stable_baselines3 import PPO, DQN, A2C\n",
    "from sb3_contrib import ARS, QRDQN, TRPO, RecurrentPPO\n",
    "\n",
    "### REWARDS #############################\n",
    "reward_function=L2RPNReward\n",
    "prefix='L2RPNReward_Simple'\n",
    "#########################################\n",
    "\n",
    "########## TRAINING PARAMETERS #################### \n",
    "N_TIMESTEP = 1_000_000\n",
    "N_EVAL_EPISODES= 5\n",
    "EVAL_FREQ = 30_000\n",
    "algorithms = [PPO, RecurrentPPO, TRPO, A2C, DQN, QRDQN, ARS]\n",
    "################################################### \n",
    "\n",
    "\n",
    "####### ENVIRONNEMENTS ###########################\n",
    "env_name = \"l2rpn_case14_sandbox\"\n",
    "env_config = {\n",
    "    \"backend_options\": {},\n",
    "    \"env_name\": \"l2rpn_case14_sandbox\", \n",
    "    \"env_is_test\": False,\n",
    "    \"act_type\": \"discrete\", \n",
    "    \"act_attr_to_keep\": [\"set_line_status_simple\", \"set_bus\"], \n",
    "    \"reward_class\": reward_function, \n",
    "}\n",
    "################################################### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc580601",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fpath(algo,prefix=''):\n",
    "    model_name = algo.__name__\n",
    "    tb_log_name = prefix + \"_\"+ f\"{model_name}\"\n",
    "    model_path = \"./\" + prefix + \"/\" f\"{model_name}\"\n",
    "    return model_name, model_path, tb_log_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062f8010",
   "metadata": {},
   "source": [
    "# Dowload environnement\n",
    "#### Split in training / validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0451974-70fe-4ee7-8aa1-4a6ebd1fa58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directories already exist. Skipping train-val-test split.\n"
     ]
    }
   ],
   "source": [
    "import grid2op\n",
    "from env_wrapper import Grid2opEnvWrapper\n",
    "\n",
    "train_dir = os.path.join(base_path, f\"{env_name}_train/\")\n",
    "val_dir = os.path.join(base_path, f\"{env_name}_val/\")\n",
    "test_dir = os.path.join(base_path, f\"{env_name}_test/\")\n",
    "\n",
    "def directories_exist(train_path, val_path, test_path):\n",
    "    return os.path.exists(train_path) and os.path.exists(val_path) and os.path.exists(test_path)\n",
    "\n",
    "# Make environment\n",
    "env = grid2op.make(env_name)\n",
    "\n",
    "if directories_exist(train_dir, val_dir, test_dir):\n",
    "    print(\"Directories already exist. Skipping train-val-test split.\")\n",
    "else:\n",
    "    print(\"Directories do not exist. Proceeding with train-val-test split.\")\n",
    "    # Perform the split\n",
    "    try:\n",
    "        nm_env_train, nm_env_val, nm_env_test = env.train_val_split_random(\n",
    "            pct_val=10, # 10% validation\n",
    "            pct_test=10, # 10% test\n",
    "            add_for_train=\"train\",\n",
    "            add_for_val=\"val\",\n",
    "            add_for_test=\"test\"\n",
    "        )\n",
    "        print(f\"Training environment created: {nm_env_train}\")\n",
    "        print(f\"Validation environment created: {nm_env_val}\")\n",
    "        print(f\"Test environment created: {nm_env_test}\")\n",
    "    except OSError as e:\n",
    "        print(f\"An error occurred during splitting: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a992296a",
   "metadata": {},
   "source": [
    "\n",
    "# Create the environments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01caaaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training environnement\n",
    "train_env_config = env_config.copy()\n",
    "train_env_config[\"data_set\"] = \"train\"\n",
    "train_env = Grid2opEnvWrapper(train_env_config)\n",
    "\n",
    "# Validation environnement\n",
    "val_env_config = env_config.copy()\n",
    "val_env_config[\"data_set\"] = \"val\"\n",
    "val_env = Grid2opEnvWrapper(val_env_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacef79d",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73270a52-cf5e-405c-bd5e-24e1f412e883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "\n",
    "monitor_env = Monitor(val_env, filename='./monitor_log', allow_early_resets=True)\n",
    "log_dir = \"./tensorboard_logs/\"\n",
    "\n",
    "# Loop over each algorithm\n",
    "for algo in algorithms:\n",
    "    # Define the model path and tensorboard log name dynamically\n",
    "    model_name, model_path, tb_log_name = fpath(algo,prefix)\n",
    "    \n",
    "    # Create Call back:\n",
    "    eval_callback = EvalCallback( \n",
    "        eval_env=monitor_env, \n",
    "        n_eval_episodes=N_EVAL_EPISODES,\n",
    "        eval_freq=EVAL_FREQ, \n",
    "        log_path = model_path, \n",
    "        best_model_save_path=model_path,\n",
    "        deterministic=True, \n",
    "        render=False,\n",
    "        verbose=0 )\n",
    "\n",
    "\n",
    "    # Initialize the algorithm with custom hyperparameters\n",
    "    policy = \"MlpPolicy\"\n",
    "    if algo == RecurrentPPO:\n",
    "        policy =  \"MlpLstmPolicy\"\n",
    "\n",
    "    sb3_algo = algo(\n",
    "        policy,\n",
    "        train_env,\n",
    "        verbose=0,\n",
    "        tensorboard_log=log_dir,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    sb3_algo.learn(\n",
    "        total_timesteps=N_TIMESTEP, \n",
    "        tb_log_name=tb_log_name,\n",
    "        callback=eval_callback\n",
    "    )\n",
    "\n",
    "    # Save the model\n",
    "    sb3_algo.save(model_path+'/'+model_name)\n",
    "\n",
    "    print(f\"{model_name} model saved at {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d4b5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF INTERRUPT\n",
    "\n",
    "# Save the model\n",
    "# sb3_algo.save(model_path+'/'+model_name)\n",
    "\n",
    "# print(f\"{model_name} model saved at {model_path}\")\n",
    "\n",
    "# algorithms=[A2C]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6dedb3",
   "metadata": {},
   "source": [
    "#### Look at evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e60105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# # Loop over each trained algorithm\n",
    "# for algo in algorithms:\n",
    "#     _, model_path, _ =  fpath(algo,prefix)\n",
    "\n",
    "#     data = np.load(model_path+'/evaluations.npz')\n",
    "\n",
    "#     results = np.mean(data['results'],axis=1)\n",
    "#     ep_len = np.mean(data['ep_lengths'],axis=1)\n",
    "#     t = data['timesteps']\n",
    "\n",
    "#     plt.figure()\n",
    "#     plt.plot(t,results/100, label='rewards/100')\n",
    "#     plt.plot(t,ep_len,label='ep_len')\n",
    "#     plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505a5c30",
   "metadata": {},
   "source": [
    "# Test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e0a355-e8f4-46b0-b888-ae79e912aa16",
   "metadata": {},
   "source": [
    "### Compare to baseline agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65912e31-be5a-4157-8be1-c660ef2f7dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grid2op.Runner import Runner\n",
    "from grid2op.Agent import DoNothingAgent, TopologyGreedy, PowerLineSwitch, RandomAgent\n",
    "from agent_wrapper import Grid2opAgentWrapper\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Testing parameters ####################\n",
    "nb_episode_test = 50\n",
    "########################################\n",
    "\n",
    "# Environnement\n",
    "# Test environnement\n",
    "test_env_config = env_config.copy()\n",
    "test_env_config[\"data_set\"] = \"test\"\n",
    "test_env_config[\"reward_class\"] = L2RPNReward # Always compare with the L2PRN reward\n",
    "test_env = Grid2opEnvWrapper(test_env_config)\n",
    "\n",
    "seeds_test_env = tuple(range(nb_episode_test))  # Seeds for the environment\n",
    "seeds_test_agent = tuple(range(nb_episode_test))  # Seeds for the agent\n",
    "ts_ep_test = tuple(range(nb_episode_test)) \n",
    "\n",
    "# Trained agents\n",
    "for algo in algorithms:\n",
    "    # Define the model path and tensorboard log name dynamically\n",
    "    model_name, model_path, _ =  fpath(algo,prefix)\n",
    "\n",
    "    # Load the trained model\n",
    "    sb3_algo_to_test = algo.load(model_path+'/'+model_name, env=test_env)\n",
    "    # sb3_algo_to_test = algo.load(model_path+'/best_model', env=test_env)\n",
    "\n",
    "    # Convert to grid2op agent\n",
    "    my_agent = Grid2opAgentWrapper(test_env, sb3_algo_to_test)\n",
    "\n",
    "    runner = Runner(**test_env._g2op_env.get_params_for_runner(),\n",
    "                    agentClass=None,\n",
    "                    agentInstance=my_agent)\n",
    "\n",
    "    res = runner.run(nb_episode=nb_episode_test,\n",
    "                    env_seeds=seeds_test_env,\n",
    "                    agent_seeds=seeds_test_agent,\n",
    "                    episode_id=ts_ep_test,\n",
    "                    add_detailed_output=True,\n",
    "                    path_save=\"test/\" + prefix + '_' + model_name \n",
    "                    )\n",
    "\n",
    "# # Baseline agents:\n",
    "baseline_agents= [RandomAgent, DoNothingAgent]\n",
    "for baseline_agent in baseline_agents : \n",
    "    \n",
    "    model_name, model_path, _ =  fpath(baseline_agent,prefix)\n",
    "    \n",
    "    runner = Runner(**test_env._g2op_env.get_params_for_runner(),\n",
    "                agentClass=baseline_agent)\n",
    "\n",
    "    res = runner.run(nb_episode=nb_episode_test,\n",
    "                 env_seeds=seeds_test_env,\n",
    "                 agent_seeds=seeds_test_agent,\n",
    "                 episode_id=ts_ep_test,\n",
    "                 add_detailed_output=True,\n",
    "                 path_save=\"test/\" + model_name\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1430ca",
   "metadata": {},
   "source": [
    "# Look at the rewards of some episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2569a8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(3,1,sharex=True)\n",
    "# plt.title('Comparison of Trained agents and Baslines')\n",
    "ax[1].set_xlabel('Episode #')\n",
    "ax[0].set_ylabel('Cumulative reward')\n",
    "\n",
    "reward_dict = dict()\n",
    "ep_dict = dict()\n",
    "\n",
    "for algo in algorithms:\n",
    "    model_name, model_path, _ =  fpath(algo,prefix)\n",
    "    files = glob.glob(\"test/\" + prefix + '_' + model_name + '/*/rewards*')\n",
    "    n_ep = np.arange(len(files))\n",
    "    ep_length=[]\n",
    "    rew_per_ep = []\n",
    "    for file in files: # 1 file = 1 episode\n",
    "        rewards = np.load( file)['data']\n",
    "        rew_per_ep.append(np.nansum(rewards))\n",
    "        ep_length.append( np.sum(~np.isnan(rewards))  )\n",
    "    ep_dict[model_name] = np.array(ep_length)\n",
    "    reward_dict[model_name] = np.array(rew_per_ep)\n",
    "    ax[0].plot(n_ep, rew_per_ep, '-',label=model_name)\n",
    "    ax[1].plot(n_ep, ep_length, '-',label=model_name)\n",
    "\n",
    "ax[0].legend()\n",
    "\n",
    "for baseline_agent in baseline_agents : \n",
    "    model_name, model_path, _ =  fpath(baseline_agent,prefix)\n",
    "    files = glob.glob(\"test/\" + model_name + '/*/rewards*')\n",
    "    n_ep = np.arange(len(files))\n",
    "    ep_length=[]\n",
    "    rew_per_ep = []\n",
    "    for file in files: # 1 file = 1 episode\n",
    "        rewards = np.load( file)['data']\n",
    "        rew_per_ep.append(np.nansum(rewards))\n",
    "        ep_length.append( np.sum(~np.isnan(rewards))  )\n",
    "    ep_dict[model_name] = np.array(ep_length)\n",
    "    reward_dict[model_name] = np.array(rew_per_ep)\n",
    "    ax[0].plot(n_ep, rew_per_ep, '-',label=model_name)\n",
    "    ax[1].plot(n_ep, ep_length, '-',label=model_name)\n",
    "ax[0].legend()\n",
    "\n",
    "ax[2].plot(n_ep, reward_dict['TRPO'] - reward_dict['DoNothingAgent'])\n",
    "ax[2].plot(n_ep, reward_dict['A2C'] - reward_dict['DoNothingAgent'])\n",
    "ax[2].plot(n_ep, reward_dict['RecurrentPPO'] - reward_dict['DoNothingAgent'])\n",
    "\n",
    "# ax[2].plot(n_ep, ep_dict['TRPO'] - ep_dict['DoNothingAgent'])\n",
    "# ax[2].plot(n_ep, ep_dict['A2C'] - ep_dict['DoNothingAgent'])\n",
    "# ax[2].plot(n_ep, ep_dict['RecurrentPPO'] - ep_dict['DoNothingAgent'])\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL_Project-henri--M3CLLTv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
