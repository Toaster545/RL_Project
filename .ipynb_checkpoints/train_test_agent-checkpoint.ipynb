{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d759dd8-6fab-49cf-962c-13f852fe4fc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2147483648"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ctypes\n",
    "ctypes.windll.kernel32.SetThreadExecutionState(0x80000002)\n",
    "# ctypes.windll.kernel32.SetThreadExecutionState(0x80000000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad634e7-e1d4-46e5-a796-7e2a292746a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO, DQN\n",
    "from sb3_contrib import TQC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0451974-70fe-4ee7-8aa1-4a6ebd1fa58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directories do not exist. Proceeding with train-val-test split.\n",
      "Training environment created: l2rpn_case14_sandbox_train\n",
      "Validation environment created: l2rpn_case14_sandbox_val\n",
      "Test environment created: l2rpn_case14_sandbox_test\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import grid2op\n",
    "from stable_baselines3 import PPO, DQN\n",
    "from sb3_contrib import TQC\n",
    "from grid2op.Runner import Runner\n",
    "from agent_wrapper import Grid2opAgentWrapper\n",
    "# from env_wrapper import Grid2opEnvWrapper\n",
    "from env_wrapper_custom import Grid2opEnvWrapper\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "from grid2op.Reward import RedispReward, L2RPNReward, EpisodeDurationReward\n",
    "\n",
    "\n",
    "env_name = \"l2rpn_case14_sandbox\"\n",
    "base_path = os.path.expanduser(\"C:/Users/henri/data_grid2op\")\n",
    "train_dir = os.path.join(base_path, f\"{env_name}_train\")\n",
    "val_dir = os.path.join(base_path, f\"{env_name}_val\")\n",
    "test_dir = os.path.join(base_path, f\"{env_name}_test\")\n",
    "\n",
    "def directories_exist(train_path, val_path, test_path):\n",
    "    return os.path.exists(train_path) and os.path.exists(val_path) and os.path.exists(test_path)\n",
    "\n",
    "# Make environment\n",
    "env = grid2op.make(env_name, reward_class=L2RPNReward)\n",
    "\n",
    "if directories_exist(train_dir, val_dir, test_dir):\n",
    "    print(\"Directories already exist. Skipping train-val-test split.\")\n",
    "else:\n",
    "    print(\"Directories do not exist. Proceeding with train-val-test split.\")\n",
    "    # Perform the split\n",
    "    try:\n",
    "        nm_env_train, nm_env_val, nm_env_test = env.train_val_split_random(\n",
    "            pct_val=10, # 10% validation\n",
    "            pct_test=10, # 10% test\n",
    "            add_for_train=\"train\",\n",
    "            add_for_val=\"val\",\n",
    "            add_for_test=\"test\"\n",
    "        )\n",
    "        print(f\"Training environment created: {nm_env_train}\")\n",
    "        print(f\"Validation environment created: {nm_env_val}\")\n",
    "        print(f\"Test environment created: {nm_env_test}\")\n",
    "    except OSError as e:\n",
    "        print(f\"An error occurred during splitting: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2fc86a-87d7-4fff-bbbb-c4f9840f3fe7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73270a52-cf5e-405c-bd5e-24e1f412e883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO, DQN, A2C\n",
    "from sb3_contrib import ARS, QRDQN, TRPO, RecurrentPPO\n",
    "from grid2op.Reward import RedispReward, L2RPNReward, EpisodeDurationReward\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "log_dir = \"./tensorboard_logs_L2RPNReward/\"\n",
    "\n",
    "env_config = {\n",
    "    # \"backend_cls\": ,\n",
    "    # \"backend_options\": {},\n",
    "    \"env_name\": \"l2rpn_case14_sandbox\", # \"l2rpn_neurips_2020_track1_small\"\n",
    "    \"env_is_test\": False,\n",
    "    # \"obs_attr_to_keep\": [\"gen_p\", \"p_or\" ,\"load_p\", \"rho\", \"line_status\"], # \"gen_q\", \"gen_v\",\n",
    "    \"act_type\": \"discrete\", # \"discrete\" \"box\" \"multi_discrete\"\n",
    "    # \"act_attr_to_keep\": [\"change_line_status\", \"set_line_status_simple\", \"set_bus\"], # set_line_status\n",
    "    \"reward_class\": L2RPNReward, # EpisodeDurationReward,\n",
    "    \"data_set\": \"train\", # for training data set train/val/test\n",
    "}\n",
    "\n",
    "# Create the environment\n",
    "vec_env = make_vec_env(lambda: Grid2opEnvWrapper(env_config), n_envs=1)\n",
    "\n",
    "# List of algorithms to train\n",
    "# algorithms = [PPO, DQN, A2C, ARS, QRDQN, TRPO, RecurrentPPO]\n",
    "algorithms = [RecurrentPPO]\n",
    "\n",
    "\n",
    "# Loop over each algorithm\n",
    "for algo in algorithms:\n",
    "    # Define the model path and tensorboard log name dynamically\n",
    "    model_name = algo.__name__\n",
    "    tb_log_name = f\"{model_name}_default_sandbox\"\n",
    "    model_path = f\"./{model_name}_default_sandbox\"\n",
    "\n",
    "    # Initialize the algorithm with custom hyperparameters\n",
    "    sb3_algo = algo(\n",
    "        # \"MlpPolicy\",\n",
    "        # \"LinearPolicy\",\n",
    "        # \"MlpLstmPolicy\",\n",
    "        vec_env,\n",
    "        verbose=0,\n",
    "        tensorboard_log=log_dir,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    sb3_algo.learn(total_timesteps=1_000_000, tb_log_name=tb_log_name)\n",
    "\n",
    "    # Save the model\n",
    "    sb3_algo.save(model_path)\n",
    "\n",
    "    print(f\"{model_name} model saved at {model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e0a355-e8f4-46b0-b888-ae79e912aa16",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65912e31-be5a-4157-8be1-c660ef2f7dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent_wrapper import Grid2opAgentWrapper\n",
    "from grid2op.Runner import Runner\n",
    "\n",
    "nb_episode_test = 10\n",
    "seeds_test_env = tuple(range(nb_episode_test))  # Seeds for the environment\n",
    "seeds_test_agent = tuple(range(nb_episode_test))  # Seeds for the agent\n",
    "# ts_ep_test = tuple(range(nb_episode_test))  # Timestamps for the episodes\n",
    "\n",
    "# uncomment those two line if the env_config is the correct for the tested agent\n",
    "# test_env_config = env_config\n",
    "# test_env_config[\"data_set\"] = \"val\"\n",
    "\n",
    "\n",
    "test_env_config = {\n",
    "    # \"backend_cls\": ,\n",
    "    # \"backend_options\": {},\n",
    "    \"env_name\": \"l2rpn_case14_sandbox\", # \"l2rpn_neurips_2020_track1_small\"\n",
    "    \"env_is_test\": False,\n",
    "    # \"obs_attr_to_keep\": [\"gen_p\", \"p_or\" ,\"load_p\", \"rho\", \"line_status\"], # \"gen_q\", \"gen_v\",\n",
    "    \"act_type\": \"discrete\", # \"discrete\" \"box\" \"multi_discrete\"\n",
    "    # \"act_attr_to_keep\": [\"change_line_status\", \"set_line_status_simple\", \"set_bus\"], # set_line_status\n",
    "    \"reward_class\": L2RPNReward,\n",
    "    \"data_set\": \"val\", # for training data set train/val/test\n",
    "}\n",
    "\n",
    "\n",
    "# test_vec_env = make_vec_env(lambda: Grid2opEnvWrapper(test_env_config), n_envs=1)\n",
    "\n",
    "test_env = Grid2opEnvWrapper(test_env_config)\n",
    "\n",
    "sb3_algo_to_test = PPO.load(model_path, env=test_env)\n",
    "\n",
    "my_agent = Grid2opAgentWrapper(test_env, sb3_algo_to_test)\n",
    "runner = Runner(**test_env._g2op_env.get_params_for_runner(),\n",
    "                agentClass=None,\n",
    "                agentInstance=my_agent)\n",
    "\n",
    "res = runner.run(nb_episode=nb_episode_test,\n",
    "                 env_seeds=seeds_test_env,\n",
    "                 agent_seeds=seeds_test_agent,\n",
    "                 # episode_id=ts_ep_test,\n",
    "                 add_detailed_output=True,\n",
    "                 path_save=model_path+\"_val\"\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791bef8d-55d2-41f7-91ea-2c38ef86ef57",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ep_data in res:\n",
    "    chron_name, cum_reward, nb_time_step, max_ts, _, ep_data = ep_data\n",
    "    print(f\"Episode {i + 1}:\")\n",
    "    print(f\"  Chronics Name: {chron_name}\")\n",
    "    print(f\"  Cumulative Reward: {cum_reward}\")\n",
    "    print(f\"  Number of Time Steps: {nb_time_step}\")\n",
    "    print(f\"  Maximum Time Steps: {max_ts}\")\n",
    "    print(\"  Detailed Data:\")\n",
    "    for t, obs in enumerate(ep_data.observations):\n",
    "        print(f\"    Time Step {t}:\")\n",
    "        print(f\"      Observation: {obs}\")\n",
    "        print(f\"      Action Taken: {ep_data.actions[t]}\")\n",
    "        print(f\"      Reward: {ep_data.rewards[t]}\")\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0403a30-4fd5-4851-9051-b39cb9a51fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "log_dir = r\"tensorboard_logs/PPO_default__sandbox_1\"\n",
    "\n",
    "data = {}\n",
    "\n",
    "for event in tf.compat.v1.train.summary_iterator(log_dir):\n",
    "    for value in event.summary.value:\n",
    "        if value.tag not in data:\n",
    "            data[value.tag] = {\"steps\": [], \"values\": []}\n",
    "        data[value.tag][\"steps\"].append(event.step)\n",
    "        data[value.tag][\"values\"].append(value.simple_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a00344-fd54-43b8-8361-6b6f878321e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7859af61-9931-4c56-bccf-ab31efb4103d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO, DQN, A2C\n",
    "from grid2op.Runner import Runner\n",
    "from agent_wrapper import Grid2opAgentWrapper\n",
    "from grid2op.Reward import EpisodeDurationReward\n",
    "\n",
    "# Number of test episodes and seeds\n",
    "nb_episode_test = 10\n",
    "seeds_test_env = tuple(range(nb_episode_test))  # Environment seeds\n",
    "seeds_test_agent = tuple(range(nb_episode_test))  # Agent seeds\n",
    "\n",
    "# Validation environment configuration\n",
    "test_env_config = {\n",
    "    \"env_name\": \"l2rpn_case14_sandbox\",  # Change as needed\n",
    "    \"env_is_test\": False,\n",
    "    \"act_type\": \"discrete\",  # \"discrete\", \"box\", \"multi_discrete\"\n",
    "    \"reward_class\": EpisodeDurationReward,\n",
    "    \"data_set\": \"val\",  # Uses validation data set\n",
    "}\n",
    "\n",
    "algorithms = [PPO, DQN, A2C, ARS, QRDQN, TRPO]\n",
    "\n",
    "# Loop through each algorithm\n",
    "for algo in algorithms:\n",
    "    # Define paths for loading models and saving validation results\n",
    "    model_name = algo.__name__\n",
    "    model_path = f\"./{model_name}_default_sandbox\"\n",
    "    validation_path = f\"{model_path}_val\"\n",
    "\n",
    "    # Create the test environment\n",
    "    test_env = Grid2opEnvWrapper(test_env_config)\n",
    "\n",
    "    # Load the trained model\n",
    "    sb3_algo_to_test = algo.load(model_path, env=test_env)\n",
    "\n",
    "    # Wrap the agent\n",
    "    my_agent = Grid2opAgentWrapper(test_env, sb3_algo_to_test)\n",
    "\n",
    "    # Initialize the Runner\n",
    "    runner = Runner(**test_env._g2op_env.get_params_for_runner(),\n",
    "                    agentClass=None,\n",
    "                    agentInstance=my_agent)\n",
    "\n",
    "    # Run validation\n",
    "    print(f\"Running validation for {model_name}...\")\n",
    "    res = runner.run(nb_episode=nb_episode_test,\n",
    "                     env_seeds=seeds_test_env,\n",
    "                     agent_seeds=seeds_test_agent,\n",
    "                     add_detailed_output=True,\n",
    "                     path_save=validation_path)\n",
    "\n",
    "    # Save or log validation results\n",
    "    print(f\"Validation results saved at {validation_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16315d1-976c-4ba3-84b0-34cedc28f2fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
