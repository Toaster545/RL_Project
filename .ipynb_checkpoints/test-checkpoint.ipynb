{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc7c3506-2db8-4afe-8868-ac14c39b75c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1ec6fc-db4a-40ba-abc2-438aa66541ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO, DQN\n",
    "from grid2op.Runner import Runner\n",
    "from agent_wrapper import Grid2opAgentWrapper\n",
    "# from env_wrapper import Grid2opEnvWrapper\n",
    "from env_wrapper_custom import Grid2opEnvWrapper\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from grid2op.Reward import EpisodeDurationReward, L2RPNReward\n",
    "\n",
    "env_name = \"l2rpn_case14_sandbox\"\n",
    "\n",
    "env = grid2op.make(env_name, reward_class=EpisodeDurationReward)\n",
    "\n",
    "nm_env_train, nm_env_val, nm_env_test = env.train_val_split_random(pct_val=10, pct_test=10)\n",
    "\n",
    "print(f\"The name of the training environment is \\\\\"{nm_env_train}\\\\\"\")\n",
    "print(f\"The name of the validation environment is \\\\\"{nm_env_val}\\\\\"\")\n",
    "print(f\"The name of the test environment is \\\\\"{nm_env_test}\\\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7bf4a6-ca2d-46a0-84aa-43f95a85b109",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"./tensorboard_logs/\"\n",
    "\n",
    "tb_log_name = \"PPO_Grid2op\"\n",
    "model_path = \"./PPO_5_3simple_sandbox\"\n",
    "\n",
    "env_config = {\n",
    "    # \"backend_cls\": ,\n",
    "    # \"backend_options\": {},\n",
    "    \"env_name\": \"l2rpn_case14_sandbox\", # \"l2rpn_neurips_2020_track1_small\"\n",
    "    \"env_is_test\": False,\n",
    "    \"obs_attr_to_keep\": [\"gen_p\", \"p_or\" ,\"load_p\", \"rho\", \"line_status\"], # \"gen_q\", \"gen_v\",\n",
    "    \"act_type\": \"discrete\", # \"discrete\" \"box\" \"multi_discrete\"\n",
    "    \"act_attr_to_keep\": [\"change_line_status\", \"set_line_status_simple\", \"set_bus\"], # set_line_status\n",
    "    \"reward_class\": EpisodeDurationReward,\n",
    "    \"data_set\": \"train\", # \n",
    "}\n",
    "\n",
    "\n",
    "vec_env = make_vec_env(lambda: Grid2opEnvWrapper(env_config), n_envs=1)\n",
    "\n",
    "#sb3_algo2 = PPO.load(model_path, env=vec_env)\n",
    "#print(\"Model loaded successfully\")\n",
    "\n",
    "\n",
    "custom_hyperparameters = {\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"n_steps\": 2048,\n",
    "    \"batch_size\": 64,\n",
    "    \"n_epochs\": 10,\n",
    "    \"gamma\": 0.99,\n",
    "    \"gae_lambda\": 0.95,\n",
    "    \"clip_range\": 0.2,\n",
    "    \"clip_range_vf\": None,\n",
    "    \"normalize_advantage\": True,\n",
    "    \"ent_coef\": 0.0,\n",
    "    \"vf_coef\": 0.5,\n",
    "    \"max_grad_norm\": 0.5,\n",
    "    \"use_sde\": False,\n",
    "    \"sde_sample_freq\": -1,\n",
    "    \"target_kl\": None,\n",
    "}\n",
    "\n",
    "# Initialize PPO with custom hyperparameters\n",
    "sb3_algo2 = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    vec_env,\n",
    "    verbose=0,\n",
    "    tensorboard_log=\"./tensorboard_logs/\",\n",
    "    **custom_hyperparameters\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "sb3_algo2.learn(total_timesteps=1000000, tb_log_name=\"PPO_5_3simple_sandbox\")\n",
    "\n",
    "# Save the model\n",
    "sb3_algo2.save(model_path)\n",
    "\n",
    "print(f\"Model saved at {model_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d65b05-f011-47ed-b82c-d7cf1554d2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(model, val_env, n_eval_episodes=10)\n",
    "print(f\"Validation Reward: {mean_reward} Â± {std_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6167db7-96f2-4950-be47-cc9ade181f10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "65ae4c10-eb54-4cac-97ad-1220c429e70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at ./qdn_default_sandbox\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO, DQN\n",
    "from grid2op.Runner import Runner\n",
    "from agent_wrapper import Grid2opAgentWrapper\n",
    "from env_wrapper import Grid2opEnvWrapper\n",
    "# from env_wrapper_custom import Grid2opEnvWrapper\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "from grid2op.Reward import EpisodeDurationReward, L2RPNReward\n",
    "\n",
    "\n",
    "log_dir = \"./tensorboard_logs/\"\n",
    "\n",
    "env_config = {\n",
    "    # \"backend_cls\": ,\n",
    "    # \"backend_options\": {},\n",
    "    \"env_name\": \"l2rpn_case14_sandbox\", # \"l2rpn_neurips_2020_track1_small\"\n",
    "    \"env_is_test\": False,\n",
    "    \"obs_attr_to_keep\": [\"gen_p\", \"p_or\" ,\"load_p\", \"rho\", \"line_status\"], # \"gen_q\", \"gen_v\",\n",
    "    \"act_type\": \"discrete\", # \"discrete\" \"box\" \"multi_discrete\"\n",
    "    \"act_attr_to_keep\": [\"change_line_status\", \"set_line_status\", \"set_bus\"]\n",
    "}\n",
    "\n",
    "# [\"rho\", \"p_or\", \"gen_p\", \"load_p\"]\n",
    "\n",
    "vec_env = make_vec_env(lambda: Grid2opEnvWrapper(env_config), n_envs=1)\n",
    "\n",
    "#sb3_algo2 = PPO.load(model_path, env=vec_env)\n",
    "#print(\"Model loaded successfully\")\n",
    "\n",
    "\n",
    "custom_hyperparameters = {\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"buffer_size\": int(1e6),\n",
    "    \"learning_starts\": 100,\n",
    "    \"batch_size\": 32,\n",
    "    \"tau\": 1.0,\n",
    "    \"gamma\": 0.99,\n",
    "    \"train_freq\": 4,\n",
    "    \"gradient_steps\": 1,\n",
    "    \"target_update_interval\": 10000,\n",
    "    \"exploration_fraction\": 0.1,\n",
    "    \"exploration_initial_eps\": 1.0,\n",
    "    \"exploration_final_eps\": 0.05,\n",
    "    \"max_grad_norm\": 10,\n",
    "}\n",
    "\n",
    "\n",
    "sb3_algo2 = DQN(\n",
    "    \"MlpPolicy\",\n",
    "    vec_env,\n",
    "    verbose=0,\n",
    "    tensorboard_log=\"./tensorboard_logs/\",\n",
    "    # **custom_hyperparameters\n",
    ")\n",
    "\n",
    "\n",
    "sb3_algo2.learn(total_timesteps=100000, tb_log_name=tb_log_name)\n",
    "\n",
    "# Save the model\n",
    "sb3_algo2.save(model_path)\n",
    "\n",
    "print(f\"Model saved at {model_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "567d0cfa-a7eb-4484-96b5-29fd918a2116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n",
      "Model saved at ./ppo_default_sandbox\n"
     ]
    }
   ],
   "source": [
    "\n",
    "custom_hyperparameters = {\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"n_steps\": 2048,\n",
    "    \"batch_size\": 64,\n",
    "    \"n_epochs\": 10,\n",
    "    \"gamma\": 0.99,\n",
    "    \"gae_lambda\": 0.95,\n",
    "    \"clip_range\": 0.2,\n",
    "    \"clip_range_vf\": None,\n",
    "    \"normalize_advantage\": True,\n",
    "    \"ent_coef\": 0.0,\n",
    "    \"vf_coef\": 0.5,\n",
    "    \"max_grad_norm\": 0.5,\n",
    "    \"use_sde\": False,\n",
    "    \"sde_sample_freq\": -1,\n",
    "    \"target_kl\": None,\n",
    "}\n",
    "\n",
    "\n",
    "sb3_algo2 = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    vec_env,\n",
    "    verbose=0,\n",
    "    tensorboard_log=\"./tensorboard_logs/\",\n",
    "    # **custom_hyperparameters\n",
    ")\n",
    "\n",
    "model_path = \"./ppo_default_sandbox\"\n",
    "\n",
    "sb3_algo2 = PPO.load(model_path, env=vec_env)\n",
    "print(\"Model loaded successfully\")\n",
    "\n",
    "sb3_algo2.learn(total_timesteps=1000000, tb_log_name=\"PPO_default__sandbox\") # DQN_default PPO_default__sandbox\n",
    "\n",
    "# Save the model\n",
    "sb3_algo2.save(model_path)\n",
    "\n",
    "print(f\"Model saved at {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0231d1d8-ec85-4643-b94d-db583fe425b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "419fa551-f1fc-48e7-9d97-4e65da63d04b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "DictReplayBuffer must be used with Dict obs space only",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 46\u001b[0m\n\u001b[0;32m     26\u001b[0m vec_env \u001b[38;5;241m=\u001b[39m make_vec_env(\u001b[38;5;28;01mlambda\u001b[39;00m: Grid2opEnvWrapper(env_config), n_envs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     29\u001b[0m custom_hyperparameters \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1e-4\u001b[39m,\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuffer_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m1e6\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_grad_norm\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m     43\u001b[0m }\n\u001b[1;32m---> 46\u001b[0m sb3_algo2 \u001b[38;5;241m=\u001b[39m \u001b[43mDQN\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMlpPolicy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvec_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreplay_buffer_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mHerReplayBuffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensorboard_log\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./tensorboard_logs/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# **custom_hyperparameters\u001b[39;49;00m\n\u001b[0;32m     53\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./qdn_her_default_sandbox\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     58\u001b[0m sb3_algo2\u001b[38;5;241m.\u001b[39mlearn(total_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100000\u001b[39m, tb_log_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDQN_HER_default_sandbox\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# DQN_default PPO_default__sandbox\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stable_baselines3\\dqn\\dqn.py:141\u001b[0m, in \u001b[0;36mDQN.__init__\u001b[1;34m(self, policy, env, learning_rate, buffer_size, learning_starts, batch_size, tau, gamma, train_freq, gradient_steps, replay_buffer_class, replay_buffer_kwargs, optimize_memory_usage, target_update_interval, exploration_fraction, exploration_initial_eps, exploration_final_eps, max_grad_norm, stats_window_size, tensorboard_log, policy_kwargs, verbose, seed, device, _init_setup_model)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexploration_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _init_setup_model:\n\u001b[1;32m--> 141\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stable_baselines3\\dqn\\dqn.py:144\u001b[0m, in \u001b[0;36mDQN._setup_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_setup_model\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 144\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_aliases()\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;66;03m# Copy running stats, see GH issue #996\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:189\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm._setup_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    187\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must pass an environment when using `HerReplayBuffer`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    188\u001b[0m         replay_buffer_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\n\u001b[1;32m--> 189\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer_class(\n\u001b[0;32m    190\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer_size,\n\u001b[0;32m    191\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space,\n\u001b[0;32m    192\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space,\n\u001b[0;32m    193\u001b[0m         device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice,\n\u001b[0;32m    194\u001b[0m         n_envs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_envs,\n\u001b[0;32m    195\u001b[0m         optimize_memory_usage\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimize_memory_usage,\n\u001b[0;32m    196\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mreplay_buffer_kwargs,\n\u001b[0;32m    197\u001b[0m     )\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_class(\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space,\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space,\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_schedule,\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_kwargs,\n\u001b[0;32m    204\u001b[0m )\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stable_baselines3\\her\\her_replay_buffer.py:64\u001b[0m, in \u001b[0;36mHerReplayBuffer.__init__\u001b[1;34m(self, buffer_size, observation_space, action_space, env, device, n_envs, optimize_memory_usage, handle_timeout_termination, n_sampled_goal, goal_selection_strategy, copy_info_dict)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     52\u001b[0m     buffer_size: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     62\u001b[0m     copy_info_dict: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     63\u001b[0m ):\n\u001b[1;32m---> 64\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobservation_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43maction_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_envs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_envs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimize_memory_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimize_memory_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhandle_timeout_termination\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhandle_timeout_termination\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;241m=\u001b[39m env\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy_info_dict \u001b[38;5;241m=\u001b[39m copy_info_dict\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stable_baselines3\\common\\buffers.py:557\u001b[0m, in \u001b[0;36mDictReplayBuffer.__init__\u001b[1;34m(self, buffer_size, observation_space, action_space, device, n_envs, optimize_memory_usage, handle_timeout_termination)\u001b[0m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    546\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    547\u001b[0m     buffer_size: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    553\u001b[0m     handle_timeout_termination: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    554\u001b[0m ):\n\u001b[0;32m    555\u001b[0m     \u001b[38;5;28msuper\u001b[39m(ReplayBuffer, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(buffer_size, observation_space, action_space, device, n_envs\u001b[38;5;241m=\u001b[39mn_envs)\n\u001b[1;32m--> 557\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_shape, \u001b[38;5;28mdict\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDictReplayBuffer must be used with Dict obs space only\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    558\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(buffer_size \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_envs, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    560\u001b[0m     \u001b[38;5;66;03m# Check that the replay buffer can fit into the memory\u001b[39;00m\n",
      "\u001b[1;31mAssertionError\u001b[0m: DictReplayBuffer must be used with Dict obs space only"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO, DQN, HerReplayBuffer\n",
    "from stable_baselines3.her.goal_selection_strategy import GoalSelectionStrategy\n",
    "from grid2op.Runner import Runner\n",
    "from agent_wrapper import Grid2opAgentWrapper\n",
    "from env_wrapper import Grid2opEnvWrapper\n",
    "# from env_wrapper_custom import Grid2opEnvWrapper\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "from grid2op.Reward import EpisodeDurationReward\n",
    "\n",
    "\n",
    "log_dir = \"./tensorboard_logs/\"\n",
    "\n",
    "env_config = {\n",
    "    # \"backend_cls\": ,\n",
    "    # \"backend_options\": {},\n",
    "    \"env_name\": \"l2rpn_case14_sandbox\", # \"l2rpn_neurips_2020_track1_small\"\n",
    "    \"env_is_test\": False,\n",
    "    \"obs_attr_to_keep\": [\"gen_p\", \"p_or\" ,\"load_p\", \"rho\", \"line_status\"], # \"gen_q\", \"gen_v\",\n",
    "    \"act_type\": \"discrete\", # \"discrete\" \"box\" \"multi_discrete\"\n",
    "    \"act_attr_to_keep\": [\"change_line_status\", \"set_line_status\", \"set_bus\"]\n",
    "}\n",
    "\n",
    "# [\"rho\", \"p_or\", \"gen_p\", \"load_p\"]\n",
    "\n",
    "vec_env = make_vec_env(lambda: Grid2opEnvWrapper(env_config), n_envs=1)\n",
    "\n",
    "\n",
    "custom_hyperparameters = {\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"buffer_size\": int(1e6),\n",
    "    \"learning_starts\": 100,\n",
    "    \"batch_size\": 32,\n",
    "    \"tau\": 1.0,\n",
    "    \"gamma\": 0.99,\n",
    "    \"train_freq\": 4,\n",
    "    \"gradient_steps\": 1,\n",
    "    \"target_update_interval\": 10000,\n",
    "    \"exploration_fraction\": 0.1,\n",
    "    \"exploration_initial_eps\": 1.0,\n",
    "    \"exploration_final_eps\": 0.05,\n",
    "    \"max_grad_norm\": 10,\n",
    "}\n",
    "\n",
    "\n",
    "sb3_algo2 = DQN(\n",
    "    \"MlpPolicy\",\n",
    "    vec_env,\n",
    "    replay_buffer_class=HerReplayBuffer,\n",
    "    verbose=0,\n",
    "    tensorboard_log=\"./tensorboard_logs/\",\n",
    "    # **custom_hyperparameters\n",
    ")\n",
    "\n",
    "\n",
    "model_path = \"./qdn_her_default_sandbox\"\n",
    "\n",
    "sb3_algo2.learn(total_timesteps=100000, tb_log_name=\"DQN_HER_default_sandbox\") # DQN_default PPO_default__sandbox\n",
    "\n",
    "# Save the model\n",
    "sb3_algo2.save(model_path)\n",
    "\n",
    "print(f\"Model saved at {model_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe14ac35-9288-46c5-aaed-6aa405e9a063",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1aae0d6-384c-45e2-82d6-4f574933f7ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4236c940-6e88-4e5b-ae79-40e1b8e753cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808be019-b31c-4478-9e91-d90bab9fd4b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de789eb0-6921-47a2-a6fe-03eae61d7073",
   "metadata": {},
   "source": [
    "### custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a971204-f408-4d23-a581-1ed4a7269b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from stable_baselines3 import PPO, DQN\n",
    "from grid2op.Runner import Runner\n",
    "from agent_wrapper import Grid2opAgentWrapper\n",
    "from env_wrapper import Grid2opEnvWrapper\n",
    "# from env_wrapper_custom import Grid2opEnvWrapper\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "from grid2op.Reward import EpisodeDurationReward, EconomicReward\n",
    "\n",
    "\n",
    "log_dir = \"./tensorboard_logs/\"\n",
    "\n",
    "env_config = {\n",
    "    # \"backend_cls\": ,\n",
    "    # \"backend_options\": {},\n",
    "    \"env_name\": \"l2rpn_case14_sandbox\", # \"l2rpn_neurips_2020_track1_small\"\n",
    "    \"env_is_test\": False,\n",
    "    \"obs_attr_to_keep\": [\"gen_p\", \"p_or\" ,\"load_p\", \"rho\", \"line_status\"], # \"gen_q\", \"gen_v\",\n",
    "    \"act_type\": \"discrete\", # \"discrete\" \"box\" \"multi_discrete\"\n",
    "    \"act_attr_to_keep\": [\"change_line_status\", \"set_line_status_simple\", \"set_bus\"], # set_line_status\n",
    "    \"reward_class\": EpisodeDurationReward,\n",
    "}\n",
    "\n",
    "\n",
    "vec_env = make_vec_env(lambda: Grid2opEnvWrapper(env_config), n_envs=1)\n",
    "\n",
    "#sb3_algo2 = PPO.load(model_path, env=vec_env)\n",
    "#print(\"Model loaded successfully\")\n",
    "\n",
    "\n",
    "custom_hyperparameters = {\n",
    "    \"learning_rate\": 1e-5,\n",
    "    \"n_steps\": 1024,\n",
    "    \"batch_size\": 32,\n",
    "    \"n_epochs\": 5,\n",
    "    \"gamma\": 0.98,\n",
    "    \"gae_lambda\": 0.9,\n",
    "    \"clip_range\": 0.3,\n",
    "    \"ent_coef\": 0.01,\n",
    "}\n",
    "\n",
    "# Initialize PPO with custom hyperparameters\n",
    "sb3_algo2 = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    vec_env,\n",
    "    verbose=0,\n",
    "    tensorboard_log=\"./tensorboard_logs/\",\n",
    "    **custom_hyperparameters\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "model_path = \"./PPO_5_3simple_sandbox\"\n",
    "\n",
    "sb3_algo2.learn(total_timesteps=1000000, tb_log_name=\"PPO_5_3simple_sandbox\")\n",
    "\n",
    "# Save the model\n",
    "sb3_algo2.save(model_path)\n",
    "\n",
    "print(f\"Model saved at {model_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3347ffc0-99b0-4858-9382-ebfaf6358db9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4294cd-1c69-4b3e-8585-84ce9f298b90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2aa7684-788e-445f-b752-9118bd76abc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Dict, List, Optional, Tuple, Type, Union\n",
    "\n",
    "from gymnasium import spaces\n",
    "import torch as th\n",
    "from torch import nn\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "\n",
    "\n",
    "class CustomNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom network for policy and value function.\n",
    "    It receives as input the features extracted by the features extractor.\n",
    "\n",
    "    :param feature_dim: dimension of the features extracted with the features_extractor (e.g. features from a CNN)\n",
    "    :param last_layer_dim_pi: (int) number of units for the last layer of the policy network\n",
    "    :param last_layer_dim_vf: (int) number of units for the last layer of the value network\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_dim: int,\n",
    "        last_layer_dim_pi: int = 64,\n",
    "        last_layer_dim_vf: int = 64,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # IMPORTANT:\n",
    "        # Save output dimensions, used to create the distributions\n",
    "        self.latent_dim_pi = last_layer_dim_pi\n",
    "        self.latent_dim_vf = last_layer_dim_vf\n",
    "\n",
    "        # Policy network\n",
    "        self.policy_net = nn.Sequential(\n",
    "            nn.Linear(feature_dim, last_layer_dim_pi), nn.ReLU()\n",
    "        )\n",
    "        # Value network\n",
    "        self.value_net = nn.Sequential(\n",
    "            nn.Linear(feature_dim, last_layer_dim_vf), nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, features: th.Tensor) -> Tuple[th.Tensor, th.Tensor]:\n",
    "        \"\"\"\n",
    "        :return: (th.Tensor, th.Tensor) latent_policy, latent_value of the specified network.\n",
    "            If all layers are shared, then ``latent_policy == latent_value``\n",
    "        \"\"\"\n",
    "        return self.forward_actor(features), self.forward_critic(features)\n",
    "\n",
    "    def forward_actor(self, features: th.Tensor) -> th.Tensor:\n",
    "        return self.policy_net(features)\n",
    "\n",
    "    def forward_critic(self, features: th.Tensor) -> th.Tensor:\n",
    "        return self.value_net(features)\n",
    "\n",
    "\n",
    "class CustomActorCriticPolicy(ActorCriticPolicy):\n",
    "    def __init__(\n",
    "        self,\n",
    "        observation_space: spaces.Space,\n",
    "        action_space: spaces.Space,\n",
    "        lr_schedule: Callable[[float], float],\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # Disable orthogonal initialization\n",
    "        kwargs[\"ortho_init\"] = False\n",
    "        super().__init__(\n",
    "            observation_space,\n",
    "            action_space,\n",
    "            lr_schedule,\n",
    "            # Pass remaining arguments to base class\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "\n",
    "    def _build_mlp_extractor(self) -> None:\n",
    "        self.mlp_extractor = CustomNetwork(self.features_dim)\n",
    "\n",
    "\n",
    "model = PPO(CustomActorCriticPolicy, \"CartPole-v1\", verbose=1)\n",
    "model.learn(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f44026-3ee5-4550-ad88-0883944b773c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a992d4e-5bab-42fb-ba76-40595d715444",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e037fd-09dd-4bba-8202-216088beb97a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345a5359-d9f2-4f8e-b1df-a49c7dc37fe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8048628-b54d-4436-a330-4d73a9d713dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a968f23f-cf81-4b29-aaeb-132a6832359b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba75bd80-2b22-4250-9b53-f2a6ec68e9e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e71121c-c690-4b2c-9857-01f87c9ebee0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe81342-7fba-4eb4-9e4a-082064b75530",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
