{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a844a17e-63c1-4d07-96ef-29f123bdd74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fpath(algo,prefix=''):\n",
    "    model_name = algo.__name__\n",
    "    tb_log_name = prefix + \"_\"+ f\"{model_name}\"\n",
    "    model_path = \"./\" + prefix + \"/\" f\"{model_name}\"\n",
    "    return model_name, model_path, tb_log_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d67c0b4-a822-4c93-9e98-17674e18acc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directories already exist. Skipping train-val-test split.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import grid2op\n",
    "from grid2op.Runner import Runner\n",
    "from agent_wrapper import Grid2opAgentWrapper\n",
    "# from env_wrapper import Grid2opEnvWrapper\n",
    "from env_wrapper_custom import Grid2opEnvWrapper\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "from grid2op.Reward import RedispReward, L2RPNReward, EpisodeDurationReward\n",
    "\n",
    "\n",
    "env_name = \"l2rpn_case14_sandbox\"\n",
    "prefix='L2RPNReward'\n",
    "\n",
    "base_path = os.path.expanduser(\"C:/Users/henri/data_grid2op\")\n",
    "train_dir = os.path.join(base_path, f\"{env_name}_train\")\n",
    "val_dir = os.path.join(base_path, f\"{env_name}_val\")\n",
    "test_dir = os.path.join(base_path, f\"{env_name}_test\")\n",
    "\n",
    "def directories_exist(train_path, val_path, test_path):\n",
    "    return os.path.exists(train_path) and os.path.exists(val_path) and os.path.exists(test_path)\n",
    "\n",
    "# Make environment\n",
    "env = grid2op.make(env_name, reward_class=L2RPNReward)\n",
    "\n",
    "if directories_exist(train_dir, val_dir, test_dir):\n",
    "    print(\"Directories already exist. Skipping train-val-test split.\")\n",
    "else:\n",
    "    print(\"Directories do not exist. Proceeding with train-val-test split.\")\n",
    "    # Perform the split\n",
    "    try:\n",
    "        nm_env_train, nm_env_val, nm_env_test = env.train_val_split_random(\n",
    "            pct_val=10, # 10% validation\n",
    "            pct_test=10, # 10% test\n",
    "            add_for_train=\"train\",\n",
    "            add_for_val=\"val\",\n",
    "            add_for_test=\"test\"\n",
    "        )\n",
    "        print(f\"Training environment created: {nm_env_train}\")\n",
    "        print(f\"Validation environment created: {nm_env_val}\")\n",
    "        print(f\"Test environment created: {nm_env_test}\")\n",
    "    except OSError as e:\n",
    "        print(f\"An error occurred during splitting: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25e2c837-cae9-44b6-b9c2-6116f2a518f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_config = {\n",
    "    # \"backend_cls\": ,\n",
    "    # \"backend_options\": {},\n",
    "    \"env_name\": \"l2rpn_case14_sandbox\", # \"l2rpn_neurips_2020_track1_small\"\n",
    "    \"env_is_test\": False,\n",
    "    # \"obs_attr_to_keep\": [\"gen_p\", \"p_or\" ,\"load_p\", \"rho\", \"line_status\"], # \"gen_q\", \"gen_v\",\n",
    "    \"act_type\": \"discrete\", # \"discrete\" \"box\" \"multi_discrete\"\n",
    "    # \"act_attr_to_keep\": [\"change_line_status\", \"set_line_status_simple\", \"set_bus\"], # set_line_status\n",
    "    \"reward_class\": L2RPNReward, # EpisodeDurationReward,\n",
    "    \"data_set\": \"train\", # for training data set train/val/test\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8453bb4a-93c1-4c14-90ba-72940797cffe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb51e0c-ab70-4859-b98f-e11c95fc135f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b55b9f-8baf-4a6d-810c-03c2018acf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grid2op.Runner import Runner\n",
    "from grid2op.Agent import DoNothingAgent, TopologyGreedy, PowerLineSwitch, RandomAgent\n",
    "from agent_wrapper import Grid2opAgentWrapper\n",
    "\n",
    "algorithms\n",
    "\n",
    "# Create test environnement\n",
    "test_env_config = env_config.copy()\n",
    "test_env_config[\"data_set\"] = \"test\"\n",
    "test_env = Grid2opEnvWrapper(test_env_config)\n",
    "\n",
    "# Testing parameters\n",
    "nb_episode_test = 50\n",
    "seeds_test_env = tuple(range(nb_episode_test))  # Seeds for the environment\n",
    "seeds_test_agent = tuple(range(nb_episode_test))  # Seeds for the agent\n",
    "\n",
    "# Trained agents\n",
    "for algo in algorithms:\n",
    "    # Define the model path and tensorboard log name dynamically\n",
    "    model_name, model_path, _ =  fpath(algo,prefix)\n",
    "\n",
    "    # Load the trained model\n",
    "    sb3_algo_to_test = algo.load(model_path+'/'+model_name, env=test_env)\n",
    "\n",
    "    # Convert to grid2op agent\n",
    "    my_agent = Grid2opAgentWrapper(test_env, sb3_algo_to_test)\n",
    "\n",
    "    runner = Runner(**test_env._g2op_env.get_params_for_runner(),\n",
    "                    agentClass=None,\n",
    "                    agentInstance=my_agent)\n",
    "\n",
    "    res = runner.run(nb_episode=nb_episode_test,\n",
    "                    env_seeds=seeds_test_env,\n",
    "                    agent_seeds=seeds_test_agent,\n",
    "                    # episode_id=ts_ep_test,\n",
    "                    add_detailed_output=True,\n",
    "                    path_save=model_path+\"/test\"\n",
    "                    )\n",
    "\n",
    "# Baseline agents:\n",
    "baseline_agents= [RandomAgent,DoNothingAgent]\n",
    "for baseline_agent in baseline_agents : \n",
    "    \n",
    "    model_name, model_path, _ =  fpath(baseline_agent,prefix)\n",
    "    \n",
    "    runner = Runner(**test_env._g2op_env.get_params_for_runner(),\n",
    "                agentClass=baseline_agent)\n",
    "\n",
    "    res = runner.run(nb_episode=nb_episode_test,\n",
    "                 env_seeds=seeds_test_env,\n",
    "                 agent_seeds=seeds_test_agent,\n",
    "                 # episode_id=ts_ep_test,\n",
    "                 add_detailed_output=True,\n",
    "                 path_save=model_path\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd49e0b1-98dc-47e6-a7e0-8337a65a3401",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(2,1,sharex=True,sharey=True)\n",
    "# plt.title('Comparison of Trained agents and Baslines')\n",
    "ax[1].set_xlabel('Episode #')\n",
    "ax[0].set_ylabel('Cumulative reward')\n",
    "cum_rewards = dict()\n",
    "for algo in algorithms:\n",
    "    model_name, model_path, _ =  fpath(algo,prefix)\n",
    "    files = glob.glob(model_path+'/test/*/rewards*')\n",
    "    n_ep = np.arange(len(files))\n",
    "    rew_per_ep = []\n",
    "    for file in files: # 1 file = 1 episode\n",
    "        rewards = np.load( file)['data']\n",
    "        rew_per_ep.append(np.nansum(rewards))\n",
    "    cum_rewards[model_name] = np.array(rew_per_ep)\n",
    "    ax[0].plot(n_ep, rew_per_ep, '-',label=model_name)\n",
    "ax[0].legend()\n",
    "\n",
    "for baseline_agent in baseline_agents : \n",
    "    model_name, model_path, _ =  fpath(baseline_agent,prefix)\n",
    "    files = glob.glob(model_path+'/*/rewards*')\n",
    "    n_ep = np.arange(len(files))\n",
    "    rew_per_ep = []\n",
    "    for file in files: # 1 file = 1 episode\n",
    "        rewards = np.load( file)['data']\n",
    "        rew_per_ep.append(np.nansum(rewards))\n",
    "    cum_rewards[model_name] = np.array(rew_per_ep)\n",
    "    ax[0].plot(n_ep, rew_per_ep, '--',label=model_name)\n",
    "ax[0].legend()\n",
    "\n",
    "\n",
    "ax[1].plot(n_ep, cum_rewards['A2C'] - cum_rewards['DoNothingAgent']  )\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d638da-4360-4a6c-a9fe-0e051ac8d710",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91850006-5986-4c0e-8ea2-b830656e313e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18908068-5f71-486d-ad58-a1eadae6829f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
